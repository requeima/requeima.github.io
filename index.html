
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0034)http://mlg.eng.cam.ac.uk/duvenaud/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="icon" type="image/x-icon" href="./RedLion_crop.ico">

<style type="text/css" media="all">
  body {font-family: 'Droid Sans', helvetica,Arial,sans-serif; }
  a:link, a:visited, a:active {text-decoration:none}
  div.container{width:80%; margin:2%; line-height:150%;}
  div.right{float:right;width:200px; margin:1em; padding:1em;}
  div.content{margin-left:4%; padding:1em;}
  div.heading-links{font-size: large; font-weight: bold}
</style>

<title>James Requeima</title>
<meta name="description" content="Academic and personal page of James Requeima.">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">
<meta http-equiv="Content-Style-Type" content="text/css">

<script type="text/javascript" async="" src="www.google-analytics.com/ga.js"></script>
<script type="text/javascript">
    function trackOutboundLink(link, category, action) { 
        try { 
        _gaq.push(['_trackEvent', category , action]); 
        } catch(err){}
    }
</script>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-8635368-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<style type="text/css">
  div.foo p {
margin-bottom:0.0em; 
margin-top:0.0em;
}
</style>

</head>

<body>
<br>
<div class="container">
<!--<div class="right"><img alt="" width="200px" src="./pictures/profile5.png"></div>-->
  <div class="right"><img alt="" width="250px" src="./Pictures/James.jpg">
  </div>
  <div class="content">
    <h1>James Ryan Requeima</h1>

    <div class="heading-links">
    </div>
       <a href="https://github.com/requeima">github</a> | <a href="https://obeymath.org/">obeymath</a> | <a href="http://jamesandantonia.blogspot.com">travel blog</a> | <a href="http://jamesandantonia.smugmug.com/">photos</a> 

    <p><b>Email: james.requeima@gmail.com </b></p>
    <p>My <a href="CV-James-Requeima-2016.pdf">Curriculum Vitae</a>.</p>



    <h2>Machine Learning</h2>
    <p>
      I'm a PhD student studying machine learning at the University of Cambridge in the <a href="http://learning.eng.cam.ac.uk/Public/WebHome">Computational and Biological Learning Lab</a>. My advisor is <a href="http://learning.eng.cam.ac.uk/Public/Turner/Turner">Dr. Richard Turner</a>. I'm interested in Bayesian optimization, meta-learning, and approximate inference methods.
    </p>

    <p>
      I'm currently a visiting student at  <a href="https://mila.quebec/en/">MILA</a> under the supervision of <a href="https://mila.quebec/en/person/bengio-yoshua/">Yoshua Bengio </a>.
    </p>      

    <p>
      Previously, I completed a Master's in machine learning, speech and language technology at the University of Cambridge where my advisor was <a href="http://mlg.eng.cam.ac.uk/zoubin/">Dr. Zoubin Ghahramani</a>. During my Master's, I worked on an information theoretic acquisition function for Bayesian optimization called IPES.
    </p>

    <h2>Invenia</h2>    
    <p>
      I’m also a researcher at <a href="http://invenia.ca/">Invenia Technical Computing</a> based in Winnipeg, Manitoba. We use machine learning techniques to forecast demand for power in the electricity grid, energy production from wind farms, and electricity prices in wholesale power markets. I helped set up our research offices in Montréal, Canada and Cambridge, England.
    </p>

    <p>
      My colleagues at Invenia and I did some analysis on electricity price time series for a couple of North American wholesale electricity markets. You can find our paper <a href="http://arxiv.org/pdf/1507.06219v1.pdf">here</a>.
    </p>

    <h2>Mathematics</h2>    
    <p>
      At one point, I was a tenured member of the <a href="http://www.dawsoncollege.qc.ca/mathematics/">Department of Mathematics at Dawson College</a> in Montréal. If you're looking for CEGEP-level materials and online resources, my colleagues and I maintain <a href="http://obeymath.org/">this website</a>.
    </p>

    <p>
      When studying mathematics, my specialization was <a href="http://en.wikipedia.org/wiki/Geometric_group_theory">geometric group theory</a>, <a href="http://en.wikipedia.org/wiki/Combinatorial_group_theory">combinatorial group theory</a>, and <a href="http://en.wikipedia.org/wiki/Algebraic_topology">algebraic topology</a>. I studied under <a href="http://en.wikipedia.org/wiki/Daniel_Wise_(mathematician)">Dani Wise</a> at McGill University, who was recently awarded a Guggenheim Fellowship and the Oswald Veblen Prize in Geometry.
    </p>

    <h2>Publications</h2>    
    <table border="0" cellspacing="2" cellpadding="2"><tbody>
    
    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1906.07697">
                <img src="Pictures/CNAPs.png" alt="Link to GPAR paper" width="160px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes</b>
        <p>
            The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPs, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.
        </p>
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon</a>,
        <a href="http://mlg.eng.cam.ac.uk/?portfolio=john-bronskill">John Bronskill</a>,
        <a href="http://www.nowozin.net/sebastian/">Sebastian Nowozin</a>
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>

        <br>
          To appear as a spotlight paper in the <em>Conference on Neural Information Processing Systems</em>, 2019.
        <br>
        <a href="https://arxiv.org/abs/1802.07182">paper</a>
        <a href="Papers/GPAR.bib">bibtex</a>
        <a href="https://github.com/cambridge-mlg/cnaps">code</a>
    </td>
    </tr>

    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1802.07182">
                <img src="Pictures/GPAR.png" alt="Link to GPAR paper" width="160px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>The Gaussian Process Autoregressive Regression Model (GPAR)</b>
        <p>
            Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Au- toregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR’s efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.
        </p>
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://willtebbutt.github.io/">Will Tebbutt</a>,
        <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        <br>
          To appear in <em>International Conference on Artificial Intelligence and Statistics</em>, 2019.
        <br>
        <a href="https://arxiv.org/abs/1802.07182">paper</a>
        | <a href="Papers/GPAR.bib">bibtex</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

        <tr>
    <td valign="top">
        <a href="https://danielflamshep.github.io/158.pdf">
                <img src="Pictures/characterizingBDNN.png" alt="Link to characterizing BDNN  paper" width="160px">
        </a>
    </td>
    <td valign="top">
        <b>Characterizing and Warping the Function space of Bayesian Neural Networks</b>
        <p>
            In this work we develop a simple method to construct priors for Bayesian neural
            networks that incorporates meaningful prior information about functions. This
            method allows us to characterize the relationship between weight space and function
            space.
        </p>
        <a href="https://danielflamshep.github.io/">Daniel Flam-Shepherd</a>,
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>,
        <br>
          NeurIPS Bayesian Deep Learning Workshop, 2018.
        <br>
        <a href="https://danielflamshep.github.io/158.pdf">paper</a>
        | <a href="Papers/characterizingBDNN.bib">bibtex</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>


    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1706.01825">
                <img src="Pictures/PDTS.png" alt="Link to PDTS paper" width="160px" height="130px">
        </a>
    </td>
    <td valign="top">
        <b>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space</b>
        <p>
            Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, ϵ-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.
        </p>
        <a href="https://jmhl.org/">José Miguel Hernández-Lobato</a>,
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://scholar.google.com/citations?user=efzneU4AAAAJ&hl=en">Edward O. Pyzer-Knapp</a>,
        <a href="https://matter.toronto.edu/about-us/">Alán Aspuru-Guzik</a>
        <br>
          International Conference on Machine Learning, 2017.
        <br>
        <a href="https://arxiv.org/abs/1706.01825">paper</a>
        | <a href="Papers/PDTS.bib">bibtex</a>
    </td>
    </tr>


    <tr><td height="20px"></td></tr>


    <tr>
    <td valign="top">
        <a href="http://bayesiandeeplearning.org/2017/papers/65.pdf">
                <img src="Pictures/BDNN_priors.png" alt="Link to BDNN priors paper" width="160px" height="130px">
        </a>
    </td>
    <td valign="top">
        <b>Mapping Gaussian Process Priors to Bayesian Neural Networks</b>
        <p>
            Currently, BNN priors are specified over network parameters with little thought given to the distributions over functions that are implied. What do N(0, 1) parameter priors look like in function space and is this a reasonable assumption? We should be thinking about priors over functions and that network architecture should be an approximation strategy for these priors. Gaussian Processes offer an elegant mechanism in the kernel to specify properties we believe our underlying function has. In this work we propose a method to, using a BNN, approximate the distribution over functions given by a GP prior.
        </p>
        <a href="https://danielflamshep.github.io/">Daniel Flam-Shepherd</a>,
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>,
        <br>
          NIPS Bayesian Deep Learning Workshop, 2017.
        <br>
        <a href="http://bayesiandeeplearning.org/2017/papers/65.pdf">paper</a>
        | <a href="Papers/BDNNpriors.bib">bibtex</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>


    <tr>
    <td valign="top">
        <a href="Papers/2016masters.pdf">
                <img src="Pictures/cam_logo.png" alt="Link to Cambridge Master’s thesis" width="160px">
        </a>
    </td>
    <td valign="top">
        <b>Master's Thesis: Integrated Predictive Entropy Search for Bayesian Optimization</b>
        <p>
            Predictive Entropy Search (PES) is an information-theoretic based acquisition function that has been demonstrated to perform well on several applications. PES harnesses our estimate of the uncertainty in our objective to recommend query points that maximize the amount of information gained about the local maximizer. It cannot, however, harness the potential information gained in our objective model hyperparameters for better recommendations. This dissertation introduces a modification to the Predictive Entropy Search acquisition function called Integrated Predictive Entropy Search (IPES) that uses a fully Bayesian treatment of our objective model hyperparameters. The IPES aquisition function is the same as the original PES aquision function except that the hyperparameters have been marginalized out of the predictive distribution and so it is able to recommend points taking into account the uncertainty and reduction in uncertainty in the hyperparameters. It can recommend queries that yield more information about the local maximizer through information gained about hyperparameters values.
        </p>
        <a href="jamesr.info/">James Requeima</a>,
        Advisor: <a href="http://mlg.eng.cam.ac.uk/zoubin/">Zoubin Ghahramani</a>
        <br>
        <a href="Papers/2016masters.pdf">paper</a>
        | <a href="Papers/2016masters.bib">bibtex</a>
        | <a href="https://github.com/requeima/thesis-code">code</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>


    <tr>
    <td valign="top">
        <a href="Papers/2009masters.pdf">
                <img src="Pictures/mcgill_logo.png" alt="Link to McGill Master’s thesis" width="160px">
        </a>
    </td>
    <td valign="top">
        <b>Master's Thesis: Relative sectional curvature in compact angled 2-complexes</b>
        <p>
            We define the notion of relative sectional curvature for 2-complexes, and prove that a compact angled 2-complex that has negative sectional curvature relative to planar sections has coherent fundamental group. We analyze a certain type of 1-complex that we call flattenable graphs Γ → X for an compact angled 2-complex X, and show that if X has nonpositive sectional curvature, and if for every flattenable graph π1(Γ) → π1(X) is finitely presented, then X has coherent fundamental group. Finally we show that if X is a compact angled 2-complex with negative sectional curvature relative to π-gons and planar sections then π1(X) is coherent. Some results are provided which are useful for creating examples of 2-complexes with these properties, or to test a 2-complex for these properties.
        </p>
        <a href="jamesr.info/">James Requeima</a>,
        Advisor: <a href="http://en.wikipedia.org/wiki/Daniel_Wise_(mathematician)">Daniel Wise</a>
        <br>
        <a href="Papers/2009masters.pdf">paper</a>
        | <a href="Papers/2009masters.bib">bibtex</a>
    </td>
    </tr>
    </tbody></table>


<tr><td height="10px"></td></tr>

</div></body></html>

