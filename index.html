
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0034)http://mlg.eng.cam.ac.uk/duvenaud/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<link rel="icon" type="image/x-icon" href="./RedLion_crop.ico">

<style type="text/css" media="all">
  a:link, a:visited, a:active {text-decoration:none}
  div.right{float:right;width:300px; margin:1em; padding:1em;}

  body {
    font-family: 'Droid Sans', Helvetica, Arial, sans-serif;
    margin: 0; /* Removes default browser margin */
    padding: 0; /* Removes any default padding */
}

div.container {
    width: 85%;
    margin: 4% auto; /* Reduce top margin from 6% to 4% */
    line-height: 150%;
}

div.content {
    margin-top: 0; /* Ensures no extra margin above content */
}

  div.heading-links{font-size: large; font-weight: bold}
</style>

<title>James Requeima</title>
<meta name="description" content="Academic and personal page of James Requeima.">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">
<meta http-equiv="Content-Style-Type" content="text/css">

<script type="text/javascript" async="" src="www.google-analytics.com/ga.js"></script>
<script type="text/javascript">
    function trackOutboundLink(link, category, action) { 
        try { 
        _gaq.push(['_trackEvent', category , action]); 
        } catch(err){}
    }
</script>

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-8635368-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<style type="text/css">
  div.foo p {
margin-bottom:0.0em; 
margin-top:0.0em;
}
</style>

</head>

<body>
<br>
<div class="container">
    <div class="right">
        <img alt="" width="300px" src="./Pictures/James2023.jpg">
    </div>
    <div class="content">
        <h1 style="margin-bottom: 0.2em;">James Ryan Requeima</h1>
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">google scholar</a> | 
        <a href="https://x.com/jamesrequeima">x</a> | 
        <a href="https://bsky.app/profile/jamesrequeima.bsky.social">bluesky</a> | 
        <a href="https://github.com/requeima">github</a> | 
        <a href="https://obeymath.org/">obeymath</a> | 
        <a href="http://jamesandantonia.blogspot.com">travel blog</a> | 
        <a href="http://jamesandantonia.smugmug.com/">photos</a>
        <p style="margin-top: 1em; margin-bottom: 0;">Email: james.requeima@gmail.com</p>
        <p style="margin-top: 0.3em; margin-bottom: 3em;">My <a href="CV-James-Requeima.pdf">Curriculum Vitae</a>.</p>

    <h2>Machine Learning</h2>
    <p>
    I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in Montreal, Canada. I'm interested in meta-learning, neural processes, and using LLMs for probabilistic regression.
    </p>
    <p>
     I was previously a postdoctoral fellow at the <a href="https://learning.cs.toronto.edu/index.html">University of Toronto</a>  and <a href="https://vectorinstitute.ai/">the Vector Institute</a> in Toronto, Canada. My supervisor is <a href="http://www.cs.toronto.edu/~duvenaud/">David Duvenaud.</a>
    </p>
    <p>
      My PhD was in machine learning at the University of Cambridge in the <a href="http://learning.eng.cam.ac.uk/Public/WebHome">Computational and Biological Learning Lab</a>. My advisor was <a href="http://learning.eng.cam.ac.uk/Public/Turner/Turner">Dr. Richard Turner</a>. 
    </p>

    <p>
      I was recently a visiting student at <a href="https://mila.quebec/en/">MILA</a> under the supervision of <a href="https://mila.quebec/en/person/bengio-yoshua/">Yoshua Bengio</a>.
    </p>      

    <p>
      Previously, I completed a Master's in machine learning, speech and language technology at the University of Cambridge where my advisor was <a href="http://mlg.eng.cam.ac.uk/zoubin/">Dr. Zoubin Ghahramani</a> <a href="https://en.wikipedia.org/wiki/Fellow_of_the_Royal_Society">FRS</a>.
    </p>

    <h2>Mathematics</h2>    
    <p>
      At one point, I was a tenured member of the <a href="http://www.dawsoncollege.qc.ca/mathematics/">Department of Mathematics at Dawson College</a> in Montréal. If you're looking for CEGEP-level materials and online resources, my colleagues and I maintain <a href="http://obeymath.org/">this website</a>. You can also see my <a href="http://obeymath.org/Main/JamesRequeima">previously taught courses.</a> 
    </p>

    <p>
      When studying mathematics, my specialization was <a href="http://en.wikipedia.org/wiki/Geometric_group_theory">geometric group theory</a>, <a href="http://en.wikipedia.org/wiki/Combinatorial_group_theory">combinatorial group theory</a>, and <a href="http://en.wikipedia.org/wiki/Algebraic_topology">algebraic topology</a>. I also completed a Master’s studying under <a href="http://en.wikipedia.org/wiki/Daniel_Wise_(mathematician)">Dani Wise</a> <a href="https://en.wikipedia.org/wiki/Fellow_of_the_Royal_Society">FRS</a> at McGill University, who was recently awarded a Guggenheim Fellowship and the Oswald Veblen Prize in Geometry.
    </p>


    <h2>Publications</h2>    
    <table border="0" cellspacing="2" cellpadding="2"><tbody>

        <tr>
            <td valign="top">
                <a href="https://www.nature.com/articles/s41586-025-08897-0">
                    <img src="Pictures/aardvark.png" alt="Link to paper" width="140px", height="120px">
                </a>
            </td>
            <td valign="top">
                <b>End-to-end data-driven weather forecasting</b>
                <p>Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are produced for temperature, mean sea level pressure, and wind speed at a geographically diverse set of weather stations, and are skillful with respect to an IFS-HRES interpolation baseline at multiple lead-times. Aardvark, by virtue of its simplicity and scalability, opens the door to a new paradigm for performing accurate and efficient data-driven medium-range weather forecasting.  </p>
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                <a href="https://github.com/stratisMarkou">Stratis Markou</a>,
                <a href="https://willtebbutt.github.io/">Will Tebbutt</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="https://www.geog.cam.ac.uk/people/herzog/">Michael Herzog</a>,
                <a href="https://www.cst.cam.ac.uk/people/ndl32">Nic Lane</a>,
                <a href="https://scholar.google.com/citations?user=Z9vzJ2cAAAAJ&hl=en&oi=sra">J. Scott Hosking</a>, 
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
                <br>
                <em> Nature</em>, 2025.
                <br>
                <a href="https://www.nature.com/articles/s41586-025-08897-0">paper</a>
            </td>
            </tr>

            <tr><td height="20px"></td></tr> 

        
            <tr>
                <td valign="top">
                    <a href="https://arxiv.org/abs/2507.05526">
                    <img src="Pictures/EstimatingInterventional.png" alt="Link to paper" width="140px", height="100px">
                    </a>
                </td>
                <td valign="top">
                    <b>Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning.</b>
                    <p>In scientific domains -- from biology to the social sciences -- many questions boil down to \textit{What effect will we observe if we intervene on a particular variable?} If the causal relationships (e.g.~a causal graph) are known, it is possible to estimate the intervention distributions. In the absence of this domain knowledge, the causal structure must be discovered from the available observational data. However, observational data are often compatible with multiple causal graphs, making methods that commit to a single structure prone to overconfidence. A principled way to manage this structural uncertainty is via Bayesian inference, which averages over a posterior distribution on possible causal structures and functional mechanisms. Unfortunately, the number of causal structures grows super-exponentially with the number of nodes in the graph, making computations intractable. We propose to circumvent these challenges by using meta-learning to create an end-to-end model: the Model-Averaged Causal Estimation Transformer Neural Process (MACE-TNP). The model is trained to predict the Bayesian model-averaged interventional posterior distribution, and its end-to-end nature bypasses the need for expensive calculations. Empirically, we demonstrate that MACE-TNP outperforms strong Bayesian baselines. Our work establishes meta-learning as a flexible and scalable paradigm for approximating complex Bayesian causal inference, that can be scaled to increasingly challenging settings in the future.</p>
                    <a href="https://scholar.google.com/citations?user=nuA78i0AAAAJ&hl=en">Anish Dhir</a>,
                    <a href="https://cbl.eng.cam.ac.uk/people/cdd43/">Cristiana Diaconu</a>,
                    <a href="https://openreview.net/profile?id=~Valentinian_Mihai_Lungu1">Valentinian Mihai Lungu</a>,
                    <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                    <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
                    <a href="https://scholar.google.co.uk/citations?user=PKcjcT4AAAAJ&hl=en">Mark van der Wilk</a>

                    <br>
                    <em>To be presented at Neural Information Processing Systems, 2025.
                    <br>
                    <a href="https://arxiv.org/abs/2507.05526">paper</a>
                </td>   
                </tr>
            

            <tr><td height="20px"></td></tr> 

        <tr>
            <td valign="top">
            <a href="https://arxiv.org/abs/2410.18959">
                <img src="Pictures/Cik.png" alt="Link to paper" width="140px",  height="80px">
            </a>
        </td>
        <td valign="top">
            <b>Context is Key: A Benchmark for Forecasting with Essential Textual Information</b>
            <p>Forecasting is a critical task in decision making across various domains. While numerical data provides a foundation, it often lacks crucial context necessary for accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge or constraints, which can be efficiently communicated through natural language. However, the ability of existing forecasting models to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. By presenting this benchmark, we aim to advance multimodal forecasting, promoting models that are both accurate and accessible to decision-makers with varied technical expertise.</p>
            <a href="https://scholar.google.ca/citations?user=6gnCkJ0AAAAJ&hl=en&oi=ao">Andrew Robert Williams*</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=4ur98b4AAAAJ">Arjun Ashok*</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=NwSnV9UAAAAJ">Étienne Marcotte</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=tdUUrS8AAAAJ">Valentina Zantedeschi</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=s0BzYvYAAAAJ">Jithendaraa Subramanian</a>,
            <a href="https://mila.quebec/en/directory/roland-riachi">Roland Riachi</a>,
            <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=71a2-WMAAAAJ">Alexandre Lacoste</a>,    
            <a href="https://scholar.google.ca/citations?hl=en&user=Avse5gIAAAAJ">Irina Rish</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=QdnjDj8AAAAJ">Nicolas Chapados</a>,
            <a href="https://scholar.google.ca/citations?hl=en&user=LR6aJcEAAAAJ">Alexandre Drouin</a>
            <br>
            <em> International Conference on Machine Learning</em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2410.18959">paper</a> | <a href="https://github.com/ServiceNow/context-is-key-forecasting">code</a> | <a href="https://servicenow.github.io/context-is-key-forecasting/v0/">benchmark</a>
        </td>
        </tr>
        
        <tr><td height="20px"></td></tr>
            <tr>
                <td valign="top">
                    <a href="https://arxiv.org/abs/2410.18959">
                    <img src="Pictures/jolt.png" alt="Link to paper" width="140px", height="110px">
                    </a>
                </td>
                <td valign="top">
                    <b>JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs.</b>
                    <p>We introduce a simple method for probabilistic predictions on tabular data based on Large Language Models (LLMs) called JoLT (Joint LLM Process for Tabular data). JoLT uses the in-context learning capabilities of LLMs to define joint distributions over tabular data conditioned on user-specified side information about the problem, exploiting the vast repository of latent problem-relevant knowledge encoded in LLMs. JoLT defines joint distributions for multiple target variables with potentially heterogeneous data types without any data conversion, data preprocessing, special handling of missing data, or model training, making it accessible and efficient for practitioners. Our experiments show that JoLT outperforms competitive methods on low-shot single-target and multi-target tabular classification and regression tasks. Furthermore, we show that JoLT can automatically handle missing data and perform data imputation by leveraging textual side information. We argue that due to its simplicity and generality, JoLT is an effective approach for a wide variety of real prediction problems.</p>
                    <a href="https://scholar.google.ca/citations?user=Zm8hNeUAAAAJ&hl=en&oi=ao">Aliaksandra Shysheya*</a>,
                    <a href="http://mlg.eng.cam.ac.uk/?portfolio=john-bronskill">John Bronskill*</a>,
                    <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                    <a href="https://shoaibahmed.github.io/">Shoaib Ahmed Siddiqui</a>,
                    <a href="https://scholar.google.com/citations?user=ynIWXnUAAAAJ&hl=en">Javier González</a>,
                    <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>,
                    <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
                    <br>
                    <em>In submission, 2025.
                    <br>
                    <a href="https://arxiv.org/abs/2502.11877">paper</a>
                </td>
                </tr>
            
                <tr><td height="20px"></td></tr> 

        <tr>
            <td valign="top">
            <a href="https://arxiv.org/abs/2412.16577">
                <img src="Pictures/BCNP.png" alt="Link to paper" width="140px", height="120px">
            </a>
        </td>
        <td valign="top">
            <b>A Meta-Learning Approach to Bayesian Causal Discovery</b>
            <p>Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges. Recent works have used meta-learning to view the problem of estimating the maximum a-posteriori causal graph as supervised learning. Yet, these methods are limited when estimating the full posterior as they fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes. Further, these methods also cannot reliably sample from the posterior over causal structures. To address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties. We compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure. </p>
            <a href="https://scholar.google.com/citations?user=nuA78i0AAAAJ&hl=en">Anish Dhir</a>,
            <a href="https://mattashman.github.io/">Matt Ashman</a>,
            <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
            <a href="https://scholar.google.co.uk/citations?user=PKcjcT4AAAAJ&hl=en">Mark van der Wilk</a>
            <br>
            <em> International Conference on Learning Representations</em>, 2025.
            <br>
            <a href="https://arxiv.org/abs/2412.16577">paper</a> | <a href="https://github.com/Anish144/CausalStructureNeuralProcess">code</a> 
        </td>
        </tr>

        <tr><td height="20px"></td></tr> 
    
        <tr>
            <td valign="top">
            <a href="https://arxiv.org/abs/2412.06472">
                <img src="Pictures/FFT.png" alt="Link to paper" width="140px",  height="100px">
            </a>
        </td>
        <td valign="top">
            <b>Food for thought: How can machine learning help better predict and understand changes in food prices?</b>
            <p>In this work, we address a lack of systematic understanding of fluctuations in food affordability in Canada. Canada's Food Price Report (CPFR) is an annual publication that predicts food inflation over the next calendar year. The published predictions are a collaborative effort between forecasting teams that each employ their own approach at Canadian Universities: Dalhousie University, the University of British Columbia, the University of Saskatchewan, and the University of Guelph/Vector Institute. While the University of Guelph/Vector Institute forecasting team has leveraged machine learning (ML) in previous reports, the most recent editions (2024--2025) have also included a human-in-the-loop approach. For the 2025 report, this focus was expanded to evaluate several different data-centric approaches to improve forecast accuracy. In this study, we evaluate how different types of forecasting models perform when estimating food price fluctuations. We also examine the sensitivity of models that curate time series data representing key factors in food pricing.</p>
            Kristina L. Kupferschmidt, 
            <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
            Mya Simpson, 
            Zohrah Varsallay, 
            Ethan Jackson, 
            Cody Kupferschmidt, 
            Sara El-Shawa, 
            Graham W. Taylor
            <br>
            <em> Preprint</em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2412.06472">paper</a>
        </td>
        </tr>
        
        <tr><td height="20px"></td></tr>

        <tr>
            <td valign="top">
            <a href="https://www.dal.ca/sites/agri-food/research/canada-s-food-price-report-2025.html">
                <img src="Pictures/CFPR2025.png" alt="Link to paper" width="140px",  height="180px">
            </a>
        </td>
        <td valign="top">
            <b>Canada's Food Price Report 2025</b>
            <p>This year marks the 15th edition of Canada's Food Price Report (CFPR), an annual publication produced collaboratively by Dalhousie University, the University of Guelph, the University of British Columbia, and the University of Saskatchewan. Each of these universities contributes to enriching the report's scope and regional expertise. The cross-country research team uses historical data sources, machine learning algorithms, and predictive analytics tools to forecast Canadian food prices. </p>
            Sylvain Charlebois, Andrea Rankin, Stacey Taylor, Bryce Cross, Vlado Keselj, Stefanie Colombo, Tiff-Annie Kenny, John Keogh, Paola A. Marignani, Janet Music, Rick Nason, Armağan Özbilge, Samantha Taylor, Evan Fraser, Ethan Jackson, Kristina Kupferschmidt, Graham Taylor, Maria Corradini, Cody Kupferschmidt, Mya Simpson, Zohra Varsally, Sara El-Shawa, Paul Uys, 
            <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
            Stuart Smyth, Claire Williams, Kate Sauser, Savannah Gleim, Kelleen Wiseman, Richard Barichello, Matias Margulis, Rebecca Feng, Janet Lord, Kim Humes, Jann McFarlane
            <br>
            <a href="https://cdn.dal.ca/content/dam/dalhousie/pdf/sites/agri-food/EN%20-%20Food%20Price%20Report%202025.pdf">Englist Report</a>
            | <a href="https://cdn.dal.ca/content/dam/dalhousie/pdf/sites/agri-food/FR%20-%20Food%20Price%20Report%202025.pdf">French Report</a>
        </td>
        </tr>


        <tr><td height="20px"></td></tr>
        
        <tr>
            <td valign="top">
            <a href="https://arxiv.org/abs/2405.12856">
                <img src="Pictures/LLMP.png" alt="Link to paper" width="140px", height="140px">
            </a>
        </td>
        <td valign="top">
            <b>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</b>
            <p>Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode. </p>
            <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>,
            <a href="http://mlg.eng.cam.ac.uk/?portfolio=john-bronskill">John Bronskill*</a>,
            <a href="https://www.cs.toronto.edu/~choidami/">Dami Choi</a>,
            <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>,
            <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
            <br>
            <em> Neural Information Processing Systems</em>, 2024.
            <em> In-context Learning Workshop ICML (Best paper award)</em>, 2024.
            <br>
            <a href="https://arxiv.org/abs/2405.12856">paper</a>
            | <a href="https://github.com/requeima/llm_processes">code</a>
        </td>
        </tr>

        <tr><td height="20px"></td></tr> 
 
        <tr>
            <td valign="top">
                <a href="https://www.arxiv.org/pdf/2408.04745">
                    <img src="Pictures/methane-2.png" alt="Link to paper" width="140px", height="120px">
                </a>
            </td>
            <td valign="top">
                <b>AI for Operational Methane Emitter Monitoring from Space</b>
                <p>Mitigating methane emissions is the fastest way to stop global warming in the short-term and buy humanity time to decarbonise. Despite the demonstrated ability of remote sensing instruments to detect methane plumes, no system has been available to routinely monitor and act on these events. We present MARS-S2L, an automated AI-driven methane emitter monitoring system for Sentinel-2 and Landsat satellite imagery deployed operationally at the United Nations Environment Programme's International Methane Emissions Observatory. We compile a global dataset of thousands of super-emission events for training and evaluation, demonstrating that MARS-S2L can skillfully monitor emissions in a diverse range of regions globally, providing a 216% improvement in mean average precision over a current state-of-the-art detection method. Running this system operationally for six months has yielded 457 near-real-time detections in 22 different countries of which 62 have already been used to provide formal notifications to governments and stakeholders.</p>
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                <a href="https://scholar.google.es/citations?user=ZwlGpiUAAAAJ&hl=en">Gonzalo Mateo-Garcia</a>,
                <a href="https://scholar.google.com/citations?user=0ABa9DMAAAAJ&hl=en">Itziar Irakulis-Loitxate</a>, 
                <a href="https://scholar.google.com/citations?user=X7AXliMAAAAJ&hl=en">Marc Watine</a>,
                <a href="https://www.linkedin.com/in/pablo-fdez-p/">Pablo Fernandez-Poblaciones</a>,  
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                <a href="https://www.researchgate.net/profile/Javier-Gorrono">Javier Gorroño</a>, 
                <a href="https://scholar.google.com/citations?user=b0Apg74AAAAJ&hl=en">Cynthia Randles</a>,
                <a href="https://www.unep.org/people/manfredi-caltagirone">Manfredi Caltagirone</a>, 
                Claudio Cifarelli
                <br>
                <em> In submission</em>, 2025.
                <br>
                <a href="https://www.arxiv.org/pdf/2408.04745">paper</a>
            </td>
            </tr>

        <tr><td height="20px"></td></tr> 

            <tr>
                <td valign="top">
                    <a href="https://openreview.net/forum?id=pftXzp6Yn3&referrer=%5Bthe%20profile%20of%20Matthew%20Ashman%5D(%2Fprofile%3Fid%3D~Matthew_Ashman1)">
                        <img src="Pictures/TETNP.png" alt="Link to paper" width="140px", height="120px">
                    </a>
                </td>
                <td valign="top">
                    <b>Translation Equivariant Transformer Neural Processes</b>
                    <p>The effectiveness of neural processes (NPs) in modelling posterior prediction maps---the mapping from data to posterior predictive distributions---has significantly improved since their inception. This improvement can be attributed to two principal factors: (1) advancements in the architecture of permutation invariant set functions, which are intrinsic to all NPs; and (2) leveraging symmetries present in the true posterior predictive map, which are problem dependent. Transformers are a notable development in permutation invariant set functions, and their utility within NPs has been demonstrated through the family of models we refer to as TNPs. Despite significant interest in TNPs, little attention has been given to incorporating symmetries. Notably, the posterior prediction maps for data that are stationary---a common assumption in spatio-temporal modelling---exhibit translation equivariance. In this paper, we introduce of a new family of translation equivariant TNPs that incorporate translation equivariance. Through an extensive range of experiments on synthetic and real-world spatio-temporal data, we demonstrate the effectiveness of TE-TNPs relative to their non-translation-equivariant counterparts and other NP baselines.  </p>
                    <a href="https://mattashman.github.io/">Matt Ashman</a>,
                    <a href="https://cbl.eng.cam.ac.uk/people/cdd43/">Cristiana Diaconu</a>,
                    <a href="https://uk.linkedin.com/in/junhyuck-kim-29308920a">Junhyuck Kim</a>,
                    <a href="https://lakeesiv.com/">Lakee Sivaraya</a>,
                    <a href="https://github.com/stratisMarkou">Stratis Markou</a>,
                    <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                    <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                    <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
                    <br>
                    International Conference on Machine Learning, 2024.
                    <br>
                    <a href="https://openreview.net/forum?id=pftXzp6Yn3&referrer=%5Bthe%20profile%20of%20Matthew%20Ashman%5D(%2Fprofile%3Fid%3D~Matthew_Ashman1)">paper</a>
                </td>
                </tr>
            
            <tr><td height="20px"></td></tr> 
            
                <tr>
                    <td valign="top">
                        <a href="https://arxiv.org/abs/2211.10381">
                            <img src="Pictures/active_sensor.png" alt="Link to paper" width="140px", height="120px">
                        </a>
                    </td>
                    <td valign="top">
                        <b>Environmental sensor placement with convolutional Gaussian neural processes</b>
                        <p>Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to place sensors in a way that maximises the informativeness of their measurements, particularly in remote regions like Antarctica. Probabilistic machine learning models can suggest informative sensor placements by finding sites that maximally reduce prediction uncertainty. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as training data, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated sensor placement experiment, the ConvGNP better predicts the performance boost obtained from new observations than GP baselines, leading to more informative sensor placements. We contrast our approach with physics-based sensor placement methods and propose future steps towards an operational sensor placement recommendation system. Our work could help to realise environmental digital twins that actively direct measurement sampling to improve the digital representation of reality. </p>
                        <a href="https://scholar.google.com/citations?user=2tT1j9QAAAAJ&hl=en&oi=ao">Tom R. Andersson</a>,
                        <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                        <a href="https://github.com/stratisMarkou">Stratis Markou</a>,
                        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                        <a href="https://www.turing.ac.uk/people/researchers/alejandro-coca-castro">Alejandro Coca-Castro</a>, 
                        <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                        <a href="https://www.metoffice.gov.uk/research/foundation/informatics-lab/informatics-lab-team">Anna-Louise Ellis</a>, 
                        <a href="https://experts.news.wisc.edu/experts/matthew-lazzara">Matthew Lazzara</a>,  
                        <a href="https://www.danjonesocean.com/">Daniel C. Jones</a>,
                        <a href="https://scholar.google.com/citations?user=Z9vzJ2cAAAAJ&hl=en&oi=sra">J. Scott Hosking</a>, 
                        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
                        
                        <br>
                        Environmental Data Science (Climate Informatics 2023 Special Issue)
                        <br>
                        <a href="https://arxiv.org/abs/2211.10381">paper</a>
                    </td>
                    </tr>
                
                    <tr><td height="20px"></td></tr>

        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2311.09848">
                    <img src="Pictures/DiffCNP.jpg" alt="Link to paper" width="140px", height="100px">
                </a>
            </td>
            <td valign="top">
                <b>Diffusion-Augmented Neural Processes</b>
                <p>Over the last few years, Neural Processes have become a useful modelling tool in many application areas, such as healthcare and climate sciences, in which data are scarce and prediction uncertainty estimates are indispensable. However, the current state of the art in the field (AR CNPs; Bruinsma et al., 2023) presents a few issues that prevent its widespread deployment. This work proposes an alternative, diffusion-based approach to NPs which, through conditioning on noised datasets, addresses many of these limitations, whilst also exceeding SOTA performance. </p>
                <a href="https://uk.linkedin.com/in/lorenzo-bonito-73082718b">Lorenzo Bonito</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                <a href="https://scholar.google.ca/citations?user=Zm8hNeUAAAAJ&hl=en&oi=ao">Aliaksandra Shysheya</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>NeurIPS Workshop on Diffusion Models</em>, 2023.
                <br>
                <a href="https://arxiv.org/abs/2311.09848">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr> 

        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2310.19932">
                    <img src="Pictures/Sim2Real.jpg" alt="Link to paper" width="140px">
                </a>
            </td>
            <td valign="top">
                <b>Sim2Real for Environmental Neural Processes</b>
                <p>Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest intraining ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-gridcontext data to make uncertainty-aware predictions at target locations. However,the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is ‘Sim2Real’: pre-trainingon reanalysis and fine-tuning on observational data. We analyse Sim2Real with a ConvCNP trained to interpolate surface air temperature over Germany, using varying numbers of weather stations for fine-tuning. On held-out weather stations, Sim2Real training substantially outperforms the same model architecture trainedonly with reanalysis data or only with station data, showing that reanalysis datacan serve as a stepping stone for learning from real observations. Sim2Real could thus enable more accurate models for weather prediction and climate monitoring. </p>
                <a href="https://scholar.google.com/citations?user=yWTTRkMAAAAJ&hl=en">Jonas Scholz</a>,
                <a href="https://scholar.google.com/citations?user=2tT1j9QAAAAJ&hl=en&oi=ao">Tom R. Andersson</a>,
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>NeurIPS Workshop on Tackling Climate Change with Machine Learning</em>, 2023.
                <br>
                <a href="https://arxiv.org/abs/2310.19932">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr> 

        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2303.14468">
                    <img src="Pictures/AR_CNP.png" alt="Link to paper" width="140px" height="100px">
                </a>
            </td>
            <td valign="top">
                <b>Autoregressive Conditional Neural Processes</b>
                <p>Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that CNPs in autoregressive (AR) mode not only significantly outperform non-AR CNPs, but are also competitive with more sophisticated models that are significantly more computationally expensive and challenging to train. This performance is remarkable given that AR CNPs are not trained to model joint dependencies. Our work provides an example of how ideas from neural distribution estimation can benefit neural processes, and motivates research into the AR deployment of other neural process models. </p>
                <a href="https://wesselb.github.io/about">Wessel Bruinsma*</a>,
                <a href="https://github.com/stratisMarkou">Stratis Markou*</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>,
                <a href="http://mlg.eng.cam.ac.uk/?portfolio=andrew-foong-yue-kwang">Andrew Y. K. Foong*</a>,
                <a href="https://scholar.google.com/citations?user=2tT1j9QAAAAJ&hl=en&oi=ao">Tom R. Andersson</a>,
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                Anthony Buonomo,
                <a href="https://scholar.google.com/citations?user=Z9vzJ2cAAAAJ&hl=en&oi=sra">J. Scott Hosking</a>, 
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>International Conference on Learning Representations</em>, 2023.
                <br>
                <a href="https://arxiv.org/abs/2303.14468">paper</a>
            </td>
            </tr>
        
            <!-- <tr><td height="20px"></td></tr> 



        <tr>
            <td valign="top">
                <a href="https://gp-seminar-series.github.io/neurips-2022/">
                    <img src="Pictures/sensor_placement.png" alt="Link to paper" width="140px" height="130px">
                </a>
            </td>
            <td valign="top">
                <b>Active Learning with Convolutional Gaussian Neural Processes for Environmental Sensor Placement</b>
                <p>Deploying environmental measurement stations can be a costly and time consuming procedure, especially in regions which are remote or otherwise difficult to access, such as Antarctica. Therefore, it is crucial that sensors are placed as efficiently as possible, maximising the informativeness of their measurements. Previous approaches for identifying salient placement locations typically model the data with a Gaussian process (GP; Williams and Rasmussen, 2006). However, designing a GP covariance which captures the complex behaviour of non-stationary spatiotemporal data is a difficult task. Further, the computational cost of these models make them challenging to scale to large environmental datasets. In this work, we explore using convolutional Gaussian neural processes (ConvGNPs; Bruinsma et al., 2021; Markou et al., 2022) to address these issues. A ConvGNP is a meta-learning model which uses a neural network to parameterise a GP predictive. Our model is data-driven, flexible, efficient, and permits gridded or off-grid input data. Using simulated surface temperature fields over Antarctica as ground truth, we show that a ConvGNP substantially outperforms a non-stationary GP baseline in terms of predictive performance. We then use the ConvGNP in a temperature sensor placement toy experiment, yielding promising results.</p>
                <a href="https://scholar.google.com/citations?user=2tT1j9QAAAAJ&hl=en&oi=ao">Tom R. Andersson</a>,
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="https://github.com/stratisMarkou">Stratis Markou</a>,
                <a href="https://www.danjonesocean.com/">Daniel C. Jones</a>,
                <a href="https://scholar.google.com/citations?user=Z9vzJ2cAAAAJ&hl=en&oi=sra">J. Scott Hosking</a>, 
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
                <a href="https://www.turing.ac.uk/people/researchers/alejandro-coca-castro">Alejandro Coca-Castro</a>, 
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                <a href="https://www.metoffice.gov.uk/research/foundation/informatics-lab/informatics-lab-team">Anna-Louise Ellis</a>, 
                <a href="https://experts.news.wisc.edu/experts/matthew-lazzara">Matthew Lazzara</a>,  
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems</em>, NeurIPS 2022.
                <br>
                <a href="https://gp-seminar-series.github.io/neurips-2022/">paper</a>
            </td>
            </tr> -->
        
            <tr><td height="20px"></td></tr> 
        

        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2207.03227">
                    <img src="Pictures/ChallengesAndPitfallsOfBayesianUnlearning" alt="Link to paper" width="150px" height="100px">
                </a>
            </td>
            <td valign="top">
                <b>Challenges and Pitfalls of Bayesian Unlearning </b>
                <p>Machine unlearning refers to the task of removing a subset of training data, thereby removing its contributions to a trained model. Approximate unlearning are one class of methods for this task which avoid the need to retrain the model from scratch on the retained data. Bayes' rule can be used to cast approximate unlearning as an inference problem where the objective is to obtain the updated posterior by dividing out the likelihood of deleted data. However this has its own set of challenges as one often doesn't have access to the exact posterior of the model parameters. In this work we examine the use of the Laplace approximation and Variational Inference to obtain the updated posterior. With a neural network trained for a regression task as the guiding example, we draw insights on the applicability of Bayesian unlearning in practical scenarios.  </p>
                <a href="https://scholar.google.ca/citations?user=llj-AHYAAAAJ&hl=en&oi=sra">Ambrish Rawat</a>, 
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>, 
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>Updatable Machine Learning Workshop</em>, ICML 2022.
                <br>
                <a href="https://arxiv.org/abs/2207.03227">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr> 
        
        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2203.08775">
                        <img src="Pictures/PracticalConditionalNeuralProcessesViaTractableDependentPredictions.png" alt="Link to paper" width="150px" height="100px">
                </a>
            </td>
            <td valign="top">
                <b>Practical Conditional Neural Processes Via Tractable Dependent Predictions</b>
                <p>Conditional Neural Processes (CNPs; Garnelo et al., 2018a) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle off-the-grid and missing data. CNPs scale to large datasets and train with ease. Due to these features, CNPs appear well-suited to tasks from environmental sciences or healthcare. Unfortunately, CNPs do not produce correlated predictions, making them fundamentally inappropriate for many estimation and decision making tasks. Predicting heat waves or floods, for example, requires modelling dependencies in temperature or precipitation over time and space. Existing approaches which model output dependencies, such as Neural Processes (NPs; Garnelo et al., 2018b) or the FullConvGNP (Bruinsma et al., 2021), are either complicated to train or prohibitively expensive. What is needed is an approach which provides dependent predictions, but is simple to train and computationally tractable. In this work, we present a new class of Neural Process models that make correlated predictions and support exact maximum likelihood training that is simple and scalable. We extend the proposed models by using invertible output transformations, to capture non-Gaussian output distributions. Our models can be used in downstream estimation tasks which require dependent function samples. By accounting for output dependencies, our models show improved predictive performance on a range of experiments with synthetic and real data. </p>
                <a href="https://github.com/stratisMarkou">Stratis Markou*</a>, 
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>, 
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="https://scholar.google.com/citations?user=oO-dzBoAAAAJ&hl=en">Anna Vaughan</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>International Conference on Learning Representations</em>, 2022.
                <br>
                <a href="https://arxiv.org/abs/2203.08775">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr>        


            <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2108.09676">
                        <img src="Pictures/EfficientGaussianNeuralProcessesforRegression.png" alt="Link to paper" width="150px" height="100px">
                </a>
            </td>
            <td valign="top">
                <b>Efficient Gaussian Neural Processes for Regression</b>
                <p>Conditional Neural Processes (CNP; Garnelo et al., 2018) are an attractive family of meta-learning models which produce well-calibrated predictions, enable fast inference at test time, and are trainable via a simple maximum likelihood procedure. A limitation of CNPs is their inability to model dependencies in the outputs. This significantly hurts predictive performance and renders it impossible to draw coherent function samples, which limits the applicability of CNPs in down-stream applications and decision making. Neural Processes (NPs; Garnelo et al., 2018) attempt to alleviate this issue by using latent variables, relying on these to model output dependencies, but introduces difficulties stemming from approximate inference. One recent alternative (Bruinsma et al.,2021), which we refer to as the FullConvGNP, models dependencies in the predictions while still being trainable via exact maximum-likelihood. Unfortunately, the FullConvGNP relies on expensive 2D-dimensional convolutions, which limit its applicability to only one-dimensional data. In this work, we present an alternative way to model output dependencies which also lends itself maximum likelihood training but, unlike the FullConvGNP, can be scaled to two- and three-dimensional data. The proposed models exhibit good performance in synthetic experiments. </p>
                <a href="https://github.com/stratisMarkou">Stratis Markou*</a>, 
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>, 
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                ICML <em>Uncertainty and Robustness in Deep Learning Workshop</em>, 2021.
                <br>
                <a href="https://arxiv.org/abs/2108.09676">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr>        


        <tr>
            <td valign="top">
                <a href="https://arxiv.org/abs/2101.03606">
                        <img src="Pictures/GNP.png" alt="Link to paper" width="150px" height="100px">
                </a>
            </td>
            <td valign="top">
                <b>The Gaussian Neural Process</b>
                <p>
                    Neural Processes (NPs; Garnelo et al., 2018) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional NPs. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process (GNP), which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.</p>
                <a href="https://wesselb.github.io/about">Wessel Bruinsma</a>,
                <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>, 
                <a href="http://mlg.eng.cam.ac.uk/?portfolio=andrew-foong-yue-kwang">Andrew Y. K. Foong</a>,
                <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon</a>,
                <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        
                <br>
                <em>Advances in Approximate Bayesian Inference Symposium</em>, 2020.
                <br>
                <a href="https://arxiv.org/abs/2101.03606">paper</a>
            </td>
            </tr>
        
            <tr><td height="20px"></td></tr>    

    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/2007.01332">
                <img src="Pictures/ConvNP.png" alt="Link to paper" width="150px" height="100px">
        </a>
    </td>
    <td valign="top">
        <b>Meta-Learning Stationary Stochastic Process Prediction with Convolutional Neural Processes</b>
        <p>
            Stationary stochastic processes (SPs) are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive SPs, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (ConvNP), which endows Neural Processes (NPs) with translation equivariance and extends convolutional conditional NPs to allow for dependencies in the predictive distribution. The latter enables ConvNPs to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard ELBO objective in NPs, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of ConvNPs on 1D regression, image completion, and various tasks with real-world spatio-temporal data.
        </p>
        <a href="http://mlg.eng.cam.ac.uk/?portfolio=andrew-foong-yue-kwang">Andrew Y. K. Foong*</a>,
        <a href="https://wesselb.github.io/about">Wessel Bruinsma*</a>,
        <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon*</a>,
        <a href="https://yanndubs.github.io/about/">Yann Dubois</a>, 
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>, <br>
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>

        <br>
        <em>Neural Information Processing Systems</em>, 2020.
        <br>
        <a href="https://arxiv.org/abs/2007.01332">paper</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

   <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/2003.03284">
                <img src="Pictures/TaskNorm.png" alt="Link to TaskNorm paper" width="150px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>TaskNorm: Rethinking Batch Normalization for Meta-Learning</b>
        <p>
            Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.
        </p>
        <a href="http://mlg.eng.cam.ac.uk/?portfolio=john-bronskill">John Bronskill*</a>,
        <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon*</a>,
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en/">James Requeima</a>,
        <a href="http://www.nowozin.net/sebastian/">Sebastian Nowozin</a>, 
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>

        <br>
        <em>International Conference on Learning Representations</em>, 2020.
        <br>
        <a href="https://arxiv.org/abs/2003.03284">paper</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

    
   <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1910.13556">
                <img src="Pictures/ConvCNP.png" alt="Link to ConvCNP paper" width="150px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>Convolutional Conditional Neural Processes</b>
        <p>
            We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space as opposed to a finite-dimensional vector space. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.
        </p>
        <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon*</a>,
        <a href="https://wesselb.github.io/about">Wessel Bruinsma*</a>,
        <a href="http://mlg.eng.cam.ac.uk/?portfolio=andrew-foong-yue-kwang">Andrew Y. K. Foong</a>,
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
        <a href="https://yanndubs.github.io/about/">Yann Dubois</a>, <br>
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>

        <br>
        <em>International Conference on Learning Representations</em>, 2020.
        <br>
        <a href="https://arxiv.org/abs/1910.13556">paper</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1906.07697">
                <img src="Pictures/CNAPs.png" alt="Link to GPAR paper" width="150px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes</b>
        <p>
            The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPs, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPs achieves state-of-the-art results on the challenging Meta-Dataset benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPs is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.
        </p>
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en/">James Requeima*</a>,
        <a href="https://gordonjo.github.io/Jekyll-Mono/about/">Jonathan Gordon*</a>,
        <a href="http://mlg.eng.cam.ac.uk/?portfolio=john-bronskill">John Bronskill*</a>,
        <a href="http://www.nowozin.net/sebastian/">Sebastian Nowozin</a>,
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>

        <br>
        <em>Conference on Neural Information Processing Systems</em>, spotlight paper, 2019.
        <br>
        <a href="https://arxiv.org/abs/1906.07697">paper</a>
        | <a href="Papers/GPAR.bib">bibtex</a>
        | <a href="https://github.com/cambridge-mlg/cnaps">code</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1802.07182">
                <img src="Pictures/GPAR.png" alt="Link to GPAR paper" width="150px" height="140px">
        </a>
    </td>
    <td valign="top">
        <b>The Gaussian Process Autoregressive Regression Model (GPAR)</b>
        <p>
            Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Au- toregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR’s efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.
        </p>
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>,
        <a href="https://willtebbutt.github.io/">Will Tebbutt*</a>,
        <a href="https://wesselb.github.io/about">Wessel Bruinsma*</a>,
        <a href="http://cbl.eng.cam.ac.uk/Public/Turner/Turner">Richard E. Turner</a>
        <br>
        <em>International Conference on Artificial Intelligence and Statistics</em>, 2019.
        <br>
        <a href="https://arxiv.org/abs/1802.07182">paper</a>
        | <a href="Papers/GPAR.bib">bibtex</a> 
        | <a href="https://github.com/wesselb/gpar">code</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

        <tr>
    <td valign="top">
        <a href="https://danielflamshep.github.io/158.pdf">
                <img src="Pictures/characterizingBDNN.png" alt="Link to characterizing BDNN  paper" width="150px">
        </a>
    </td>
    <td valign="top">
        <b>Characterizing and Warping the Function space of Bayesian Neural Networks</b>
        <p>
            In this work we develop a simple method to construct priors for Bayesian neural
            networks that incorporates meaningful prior information about functions. This
            method allows us to characterize the relationship between weight space and function
            space.
        </p>
        <a href="https://danielflamshep.github.io/">Daniel Flam-Shepherd</a>,
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
        <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>,
        <br>
          NeurIPS Bayesian Deep Learning Workshop, 2018.
        <br>
        <a href="https://danielflamshep.github.io/158.pdf">paper</a>
        | <a href="Papers/characterizingBDNN.bib">bibtex</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>


    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1706.01825">
                <img src="Pictures/PDTS.png" alt="Link to PDTS paper" width="150px" height="130px">
        </a>
    </td>
    <td valign="top">
        <b>Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space</b>
        <p>
            Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, ϵ-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.
        </p>
        <a href="https://jmhl.org/">José Miguel Hernández-Lobato*</a>,
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima*</a>,
        <a href="https://scholar.google.com/citations?user=efzneU4AAAAJ&hl=en">Edward O. Pyzer-Knapp</a>,
        <a href="https://matter.toronto.edu/about-us/">Alán Aspuru-Guzik</a>
        <br>
          International Conference on Machine Learning, 2017.
        <br>
        <a href="https://arxiv.org/abs/1706.01825">paper</a>
        | <a href="Papers/PDTS.bib">bibtex</a>
    </td>
    </tr>


    <tr><td height="20px"></td></tr>

    <tr>
    <td valign="top">
        <a href="http://bayesiandeeplearning.org/2017/papers/65.pdf">
                <img src="Pictures/BDNN_priors.png" alt="Link to BDNN priors paper" width="150px" height="130px">
        </a>
    </td>
    <td valign="top">
        <b>Mapping Gaussian Process Priors to Bayesian Neural Networks</b>
        <p>
            Currently, BNN priors are specified over network parameters with little thought given to the distributions over functions that are implied. What do N(0, 1) parameter priors look like in function space and is this a reasonable assumption? We should be thinking about priors over functions and that network architecture should be an approximation strategy for these priors. Gaussian Processes offer an elegant mechanism in the kernel to specify properties we believe our underlying function has. In this work we propose a method to, using a BNN, approximate the distribution over functions given by a GP prior.
        </p>
        <a href="https://danielflamshep.github.io/">Daniel Flam-Shepherd</a>,
        <a href="https://scholar.google.ca/citations?user=fyYPWBsAAAAJ&hl=en">James Requeima</a>,
        <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
        <br>
          NeurIPS Bayesian Deep Learning Workshop, 2017.
        <br>
        <a href="http://bayesiandeeplearning.org/2017/papers/65.pdf">paper</a>
        | <a href="Papers/BDNNpriors.bib">bibtex</a>
    </td>
    </tr>

    <tr><td height="20px"></td></tr>

    <tr>
    <td valign="top">
        <a href="https://arxiv.org/abs/1507.06219">
                <img src="Pictures/PJM_prices.png" alt="Link to wholesale electricity prices paper" width="150px" height="150px">
        </a>
    </td>
    <td valign="top">
        <b>Multi-scaling of wholesale electricity prices</b>
        <p>
            We empirically analyze the most volatile component of the electricity price time series from two North-American wholesale electricity markets. We show that these time series exhibit fluctuations which are not described by a Brownian Motion, as they show multi-scaling, high Hurst exponents and sharp price movements. We use the generalized Hurst exponent (GHE, H(q)) to show that although these time-series have strong cyclical components, the fluctuations exhibit persistent behaviour, i.e., H(q)>0.5. We investigate the effectiveness of the GHE as a predictive tool in a simple linear forecasting model, and study the forecast error as a function of H(q), with q=1 and q=2. Our results suggest that the GHE can be used as prediction tool for these time series when the Hurst exponent is dynamically evaluated on rolling time windows of size ≈50−100 hours. These results are also compared to the case in which the cyclical components have been subtracted from the time series, showing the importance of cyclicality in the prediction power of the Hurst exponent.
        </p>
        <a href="https://sites.google.com/site/francescocaravelli/">Francesco Caravelli</a>,
        <a href="jamesr.info/">James Requeima</a>,
        <a href="https://uk.linkedin.com/in/cozmin-ududec-471a2954">Cozmin Ududec</a>,
        Ali Ashtari, 
        Tiziana Di Matteo, 
        Tomaso Aste
        <br>
          arXiv e-print.
        <br>
        <a href="https://arxiv.org/abs/1507.06219">paper</a>
        | <a href="Papers/caravelli2015multiscaling.bib">bibtex</a>
    </td>
    </tr>

    
    </tbody></table>

    <h2>Theses</h2>    
    <table border="0" cellspacing="2" cellpadding="2"><tbody>
        <tr><td height="20px"></td></tr>

        <tr>
            <td valign="top">
                <a href="Papers/Requeima_PhD_Thesis.pdf">
                        <img src="Pictures/cam_logo.png" alt="Link to Cambridge PhD thesis" width="150px">
                </a>
            </td>
            <td valign="top">
                <b>PhD Thesis. The Neural Processes Family: Translation Equivariance and Output Dependencies</b>
                <p>
                    Neural processes are
                    a family of meta-learning models which combine the flexibility of deep learning with
                    the uncertainty awareness of probabilistic models. Neural processes produce well-calibrated predictions,
                    enable fast inference at test time, and have flexible data-handling properties that make
                    them a good candidate for messy real-world datasets and applications.
                    This thesis focuses on addressing two shortcomings when applying neural
                    processes to real-world applications by i) incorporating translation equivariance into
                    the architecture of neural processes rather than requiring the model to learn this
                    inductive bias directly from data and ii) developing methods for neural processes
                    to parametrize rich predictive distributions that can model dependencies between
                    output-space variables and produce coherent samples.
                </p>
                <a href="jamesr.info/">James Requeima</a>,
                Advisors: <a href="https://www.eng.cam.ac.uk/profiles/ret26">Richard E. Turner</a>, 
                <a href="https://jmhl.org/">José Miguel Hernández-Lobato</a>
                
                <br>
                <a href="Papers/Requeima_PhD_Thesis.pdf">paper</a>
                | <a href="Papers/Requeima_PhD_Thesis.bib">bibtex</a>
            </td>
        </tr>

        <tr><td height="20px"></td></tr>

        <tr>
            <td valign="top">
                <a href="Papers/2016masters.pdf">
                        <img src="Pictures/cam_logo.png" alt="Link to Cambridge Master’s thesis" width="150px">
                </a>
            </td>
            <td valign="top">
                <b>Master's Thesis. Integrated Predictive Entropy Search for Bayesian Optimization</b>
                <p>
                    Predictive Entropy Search (PES) is an information-theoretic based acquisition function that has been demonstrated to perform well on several applications. PES harnesses our estimate of the uncertainty in our objective to recommend query points that maximize the amount of information gained about the local maximizer. It cannot, however, harness the potential information gained in our objective model hyperparameters for better recommendations. This dissertation introduces a modification to the Predictive Entropy Search acquisition function called Integrated Predictive Entropy Search (IPES) that uses a fully Bayesian treatment of our objective model hyperparameters. The IPES aquisition function is the same as the original PES aquision function except that the hyperparameters have been marginalized out of the predictive distribution and so it is able to recommend points taking into account the uncertainty and reduction in uncertainty in the hyperparameters. It can recommend queries that yield more information about the local maximizer through information gained about hyperparameters values.
                </p>
                <a href="jamesr.info/">James Requeima</a>,
                Advisor: <a href="http://mlg.eng.cam.ac.uk/zoubin/">Zoubin Ghahramani</a>
                <br>
                <a href="Papers/2016masters.pdf">paper</a>
                | <a href="Papers/2016masters.bib">bibtex</a>
                | <a href="https://github.com/requeima/thesis-code">code</a>
            </td>
        </tr>

        <tr><td height="20px"></td></tr>

        <tr>
            <td valign="top">
                <a href="Papers/2009masters.pdf">
                        <img src="Pictures/mcgill_logo.png" alt="Link to McGill Master’s thesis" width="150px">
                </a>
            </td>
            <td valign="top">
                <b>Master's Thesis. Relative sectional curvature in compact angled 2-complexes</b>
                <p>
                    We define the notion of relative sectional curvature for 2-complexes, and prove that a compact angled 2-complex that has negative sectional curvature relative to planar sections has coherent fundamental group. We analyze a certain type of 1-complex that we call flattenable graphs Γ → X for an compact angled 2-complex X, and show that if X has nonpositive sectional curvature, and if for every flattenable graph π1(Γ) → π1(X) is finitely presented, then X has a coherent fundamental group. Finally we show that if X is a compact angled 2-complex with negative sectional curvature relative to π-gons and planar sections then π1(X) is coherent. Some results are provided which are useful for creating examples of 2-complexes with these properties, or to test a 2-complex for these properties.
                </p>
                <a href="jamesr.info/">James Requeima</a>,
                Advisor: <a href="http://en.wikipedia.org/wiki/Daniel_Wise_(mathematician)">Daniel Wise</a>
                <br>
                <a href="Papers/2009masters.pdf">paper</a>
                | <a href="Papers/2009masters.bib">bibtex</a>
            </td>
            </tr>

        
    </tbody></table>    


<tr><td height="10px"></td></tr>

</div></body></html>

